- name: Create Azure Resources for Kubernetes Cluster and Grafana Monitoring
  hosts: localhost
  connection: local
  tasks:
    - name: Create resource group
      azure_rm_resourcegroup:
        name: A2S2_groep_04
        location: westeurope

    - name: Create virtual network
      azure_rm_virtualnetwork:
        resource_group: A2S2_groep_04
        name: KubeVnet
        address_prefixes: "10.0.0.0/16"

    - name: Add subnet
      azure_rm_subnet:
        resource_group: A2S2_groep_04
        name: KubeSubnet
        address_prefix: "10.0.1.0/24"
        virtual_network: KubeVnet

    - name: Create public IP for Grafana
      azure_rm_publicipaddress:
        resource_group: A2S2_groep_04
        allocation_method: Static
        name: GrafanaPublicIP
      register: grafana_ip

    - name: Create public IP for KubeMaster
      azure_rm_publicipaddress:
        resource_group: A2S2_groep_04
        allocation_method: Static
        name: KubeMasterPublicIP
      register: kube_master_ip

    - name: Create public IP for KubeWorker
      azure_rm_publicipaddress:
        resource_group: A2S2_groep_04
        allocation_method: Static
        name: KubeWorkerPublicIP
      register: kube_worker_ip

    - name: Create public IP for HAProxy
      azure_rm_publicipaddress:
        resource_group: A2S2_groep_04
        allocation_method: Static
        name: HAProxyPublicIP
      register: haproxy_ip

    - name: Create Network Security Group for Grafana
      azure_rm_securitygroup:
        resource_group: A2S2_groep_04
        name: GrafanaNetworkSecurityGroup
        rules:
          - name: SSH
            protocol: Tcp
            destination_port_range: 22
            access: Allow
            priority: 1001
            direction: Inbound
          - name: Grafana
            protocol: Tcp
            destination_port_range: 3000
            access: Allow
            priority: 1002
            direction: Inbound

    - name: Create Network Security Group for KubeMaster
      azure_rm_securitygroup:
        resource_group: A2S2_groep_04
        name: KubeMasterNSG
        rules:
          - name: SSH
            protocol: Tcp
            destination_port_range: 22
            access: Allow
            priority: 1001
            direction: Inbound
          - name: Kubernetes API
            protocol: Tcp
            destination_port_range: 6443
            access: Allow
            priority: 1002
            direction: Inbound

    - name: Create Network Security Group for KubeWorker
      azure_rm_securitygroup:
        resource_group: A2S2_groep_04
        name: KubeWorkerNSG
        rules:
          - name: SSH
            protocol: Tcp
            destination_port_range: 22
            access: Allow
            priority: 1001
            direction: Inbound

    - name: Create Network Security Group for HAProxy
      azure_rm_securitygroup:
        resource_group: A2S2_groep_04
        name: HAProxyNetworkSecurityGroup
        rules:
          - name: SSH
            protocol: Tcp
            destination_port_range: 22
            access: Allow
            priority: 1001
            direction: Inbound
          - name: Load Balancer HTTP
            protocol: Tcp
            destination_port_range: 80
            access: Allow
            priority: 1002
            direction: Inbound
          - name: Load Balancer MySQL
            protocol: Tcp
            destination_port_range: 3306
            access: Allow
            priority: 1003
            direction: Inbound

    - name: Create virtual network interface for Grafana
      azure_rm_networkinterface:
        resource_group: A2S2_groep_04
        name: GrafanaNIC
        virtual_network: KubeVnet
        subnet: KubeSubnet
        security_group: GrafanaNetworkSecurityGroup
        ip_forwarding: true
        ip_configurations:
          - name: GrafanaNICConfig
            public_ip_address_name: GrafanaPublicIP
            primary: true

    - name: Create virtual network interface for KubeMaster
      azure_rm_networkinterface:
        resource_group: A2S2_groep_04
        name: KubeMasterNIC
        virtual_network: KubeVnet
        subnet: KubeSubnet
        security_group: KubeMasterNSG
        ip_forwarding: true
        ip_configurations:
          - name: KubeMasterNICConfig
            public_ip_address_name: KubeMasterPublicIP
            primary: true

    - name: Create virtual network interface for KubeWorker
      azure_rm_networkinterface:
        resource_group: A2S2_groep_04
        name: KubeWorkerNIC
        virtual_network: KubeVnet
        subnet: KubeSubnet
        security_group: KubeWorkerNSG
        ip_forwarding: true
        ip_configurations:
          - name: KubeWorkerNICConfig
            public_ip_address_name: KubeWorkerPublicIP
            primary: true

    - name: Create virtual network interface for HAProxy
      azure_rm_networkinterface:
        resource_group: A2S2_groep_04
        name: HAProxyNIC
        virtual_network: KubeVnet
        subnet: KubeSubnet
        security_group: HAProxyNetworkSecurityGroup
        ip_forwarding: true
        ip_configurations:
          - name: HAProxyNICConfig
            public_ip_address_name: HAProxyPublicIP
            primary: true

    - name: Create Grafana VM
      azure_rm_virtualmachine:
        resource_group: A2S2_groep_04
        name: Grafana
        vm_size: Standard_B2ms
        admin_username: groep04_admin
        ssh_password_enabled: false
        ssh_public_keys:
          - path: /home/groep04_admin/.ssh/authorized_keys
            key_data: "ssh-dss AAAAB3NzaC1kc3MAAACBAI1OApEiXh3lWAv+QuPu8kxYBJPl+FvIV/z1o2qoCvlwpYyRQ+783nNaTjECOxMiwqk7MtXZ13KkvD91I2M0RhNSoFGH50Hx2nBuxx1D3+1vtRVzH2pAezrX9dZxk7rfK71p3sD4uL8NN5/6yNg9P1jVXygWmOa5LmD1xKN+WNQpAAAAFQCC4TLEiw6/eSoLrQfGMNVreIw0RQAAAIB/5novtYmkKgNPUgSI6ukOlS0fLmd/92yExdCW8p1m2dB95Lep6dZ4rIKImK7aGoXc/5Q0qDeOcMQGNsM3UHRBiEk4lw9mmAHyLHK6a1RG6ljFwm90odX13txIAwweeMPy6l6O6TYGT+CXCFTz4TwSULbZlGt0qmX/xZSEwik5ZgAAAIBC3y/dGeTXgO244UuiY525O9U+ypX6K8OoHS44xM8gAwhiEgJBS03x798vP9+Y70+ZHNFz0hDKwXqWynau0cOVUWnMJdajaUzYXhZm3bBgrbUQBrIMfGHO1EtaLZtZrlkC7vlXc/gr/KNvIgzDcnUm7jKLyhIa7dRX6aVAa6KzZg== root@ansible"
        network_interfaces: GrafanaNIC
        managed_disk_type: Standard_LRS
        image:
          offer: 0001-com-ubuntu-server-jammy
          publisher: Canonical
          sku: 22_04-lts
          version: latest
        custom_data: |
          #cloud-config
          package_update: true
          package_upgrade: true
          packages:
            - apt-transport-https
            - software-properties-common
            - curl
            - wget
          runcmd:
            - curl -fsSL https://packages.grafana.com/gpg.key | apt-key add -
            - add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
            - apt-get update && apt-get install -y grafana
            - systemctl start grafana-server
            - systemctl enable grafana-server
            - curl -LO https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
            - tar -xzf prometheus-2.45.0.linux-amd64.tar.gz -C /usr/local/bin
            - mv /usr/local/bin/prometheus-2.45.0.linux-amd64 /usr/local/bin/prometheus
            - curl -o /etc/prometheus.yml https://raw.githubusercontent.com/prometheus/prometheus/main/documentation/examples/prometheus-kubernetes.yml
            - systemctl start prometheus
            - systemctl enable prometheus
            - grafana-cli plugins install grafana-piechart-panel
            - grafana-cli plugins install prometheus-datasource
            - systemctl restart grafana-server
            - curl -X POST http://admin:admin@localhost:3000/api/datasources \
              -H "Content-Type: application/json" \
              -d '{
                    "name": "Prometheus",
                    "type": "prometheus",
                    "url": "http://localhost:9090",
                    "access": "proxy",
                    "isDefault": true
                  }'
        write_files:
          - path: /etc/prometheus/prometheus.yml
            content: |
              global:
                scrape_interval: 15s

              scrape_configs:
                - job_name: 'kubernetes_nodes'
                  static_configs:
                    - targets:
                        - KubeMaster:9100
                        - KubeWorker:9100

- name: Create KubeMaster VM and Deploy Applications
  azure_rm_virtualmachine:
    resource_group: A2S2_groep_04
    name: KubeMaster
    vm_size: Standard_B2ms
    admin_username: groep04_admin
    ssh_password_enabled: false
    ssh_public_keys:
      - path: /home/groep04_admin/.ssh/authorized_keys
# hier moet nog key worden angepast         SHA256:FlIqlgbmlaSTVK32cV2V6qFmcQCWlbL11fywzgVLW1w  (user)
        key_data: "ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBD/6Ny9CM88WjqN7AeQQy8EaZQcnxvyEEuN03qhQDfHu++HISwbU/XW7dG4RoTLblZqhy7gOHlI0C0Yi2k/Q3pU= root@ansible"
    network_interfaces: KubeMasterNIC
    managed_disk_type: Standard_LRS
    image:
      offer: 0001-com-ubuntu-server-jammy
      publisher: Canonical
      sku: 22_04-lts
      version: latest
    custom_data: |
      #cloud-config
      package_update: true
      package_upgrade: true
      packages:
        - apt-transport-https
        - ca-certificates
        - curl
        - kubeadm
        - kubectl
        - kubelet
      runcmd:
        - kubeadm init --pod-network-cidr=10.244.0.0/16
        - mkdir -p /home/groep04_admin/.kube
        - cp -i /etc/kubernetes/admin.conf /home/groep04_admin/.kube/config
        - chown -R groep04_admin:groep04_admin /home/groep04_admin/.kube
        - kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
        - kubectl create deployment nginx --image=nginx
        - kubectl expose deployment nginx --port=80 --target-port=80 --type=ClusterIP
        - kubectl create deployment mariadb --image=mariadb
        - kubectl expose deployment mariadb --port=3306 --target-port=3306 --type=ClusterIP
        - kubectl set env deployment/mariadb MYSQL_ROOT_PASSWORD=root
        - apt-get install -y prometheus-node-exporter
        - systemctl enable prometheus-node-exporter
        - systemctl start prometheus-node-exporter


- name: Fetch join command from KubeMaster
  shell: cat /root/kube_join_command.sh
  register: kube_join_command
  delegate_to: KubeMaster

- name: Create KubeWorker VM and Join Kubernetes Cluster
  azure_rm_virtualmachine:
    resource_group: A2S2_groep_04
    name: KubeWorker
    vm_size: Standard_B2ms
    admin_username: groep04_admin
    ssh_password_enabled: false
    ssh_public_keys:
      - path: /home/groep04_admin/.ssh/authorized_keys
        key_data: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILblctV0E1azRBM3ORHWRz798XqSsdnccNnY7yuPmCNH root@ansible"
    network_interfaces: KubeWorkerNIC
    managed_disk_type: Standard_LRS
    image:
      offer: 0001-com-ubuntu-server-jammy
      publisher: Canonical
      sku: 22_04-lts
      version: latest
    custom_data: |
      #cloud-config
      package_update: true
      package_upgrade: true
      packages:
        - apt-transport-https
        - ca-certificates
        - curl
        - kubeadm
        - kubectl
        - kubelet
      runcmd:
        - "{{ kube_join_command.stdout }}"

- name: Install Flannel Network Plugin
  shell: |
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
  delegate_to: KubeMaster

  - name: Create HAProxy VM
    azure_rm_virtualmachine:
      resource_group: A2S2_groep_04
      name: HAProxy
      vm_size: Standard_B2ms
      admin_username: groep04_admin
      ssh_password_enabled: false
      ssh_public_keys:
        - path: /home/groep04_admin/.ssh/authorized_keys
          key_data: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCmBK7ZQJdBiDFcJCzcIAIX5azgt3U/qEKgiOKspcv3zLtodeoqFeCzVA6gLkbknAhNFq2czjospX/qJ1vg3ASt/ccX17dnr1E97KV2P0FoawOS5jFqW1Nn3F4uqVpKAsUsSgmLQuRLY72NnY6K8DHuCnYvTTnQMf5h8ZQNinM5+1cw9IXy8oha2ev+lFixYsKqh81gAmLyfhfcEEsCdH8Vv7fz6FWDoXoBAY/gFcJJP2Xm6q9cMuC6f7Rl1swiY+JYlAU6sLTn9Y+Sscmfy9/plLRJYSQRH2vKjgqTUgHOs5bqyydufeO8/eEjszLSrFAmfg/MasTOHVv+q+fWnTRQJqGB8CiH3Cu3eMcO0nmGA2IXWgXQEb8+mn4s/IQah0ufts7yCFONxcM67W2xIJ2Ij8Ce99s8ORhfLNV2uH0iFsGxMp7MsmGX8E99i3xNMzAVdkYM55/USCHFBdbzD2yIYcj2K0RdIgt/AxGbtmDgR7qv1f4C7UGueCfQCGfzUo8= root@ansible"
      network_interfaces: HAProxyNIC
      managed_disk_type: Standard_LRS
      image:
        offer: 0001-com-ubuntu-server-jammy
        publisher: Canonical
        sku: 22_04-lts
        version: latest
      custom_data: |
        #cloud-config
        package_update: true
        package_upgrade: true
        packages:
          - haproxy
          - socat
          - curl
        write_files:
          - path: /etc/haproxy/haproxy.cfg
            content: |
              global
                log /dev/log local0
                log /dev/log local1 notice
                maxconn 4096
                daemon

              defaults
                log global
                option tcplog
                option httplog
                option redispatch
                retries 3
                timeout connect 5s
                timeout client  50s
                timeout server  50s

              frontend http_front
                bind *:80
                default_backend web_servers

              backend web_servers
                balance roundrobin
                server-template webserver- 5 nginx.default.svc.cluster.local:80 check

              frontend db_front
                bind *:3306
                default_backend db_servers

              backend db_servers
                balance roundrobin
                server-template dbserver- 5 mariadb.default.svc.cluster.local:3306 check
        runcmd:
          - systemctl restart haproxy
          - systemctl enable haproxy
